# -*- coding: utf-8 -*-
"""CNN .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XZYbrTX9ITv2EFZ4uIAdNDXHpqPyNa1S

# Importing necessary libraries
"""

import nltk, os
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer,PorterStemmer
from nltk.corpus import stopwords
import re, string
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from nltk import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D, SimpleRNN
from keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

"""# Loading & Reading the Dataset"""

# Clone the dataset repository from GitHub
!git clone https://github.com/chanchalIITP/AuthorIdentification.git
# Load the dataset from GitHub
files_path = r'/content/AuthorIdentification/Dataset/Dataset_with_varying_number_of_tweets/1000_tweets_per_user.csv'
# Read the dataset from a CSV file
dataset = pd.read_csv(files_path)

#dataset = pd.read_csv('/content/1000_tweets_per_user.csv')

# Display the shape of the dataset
dataset.shape

# Display the first few rows of the dataset
dataset.head()

dataset.iloc[0][0]

"""# Preprocessing"""

#nltk.download('stopwords')
#nltk.download('wordnet')
#lemmatizer = WordNetLemmatizer()
#stemmer = PorterStemmer()
#EPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
#BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
#STOPWORDS = list((stopwords.words('english')))
# Function to preprocess the text data by converting it to lowercase.
def preprocess(sentence):
    sentence=str(sentence)
    sentence = sentence.lower()
    return sentence

# Apply preprocessing to the text column of the dataset
dataset.iloc[:, 0] = dataset.iloc[:,0].map(lambda s:preprocess(s))

dataset.iloc[:,0][0]

# Extract unique classes from the dataset
len(dataset['1'].unique()), dataset['1'].unique()

# Create dictionaries for label encoding

    classes = dataset['1'].unique()

    d_encoder = {}
    d_decoder = {}

    count = 0
    for i in classes:
      if i not in d_encoder.keys():
        d_encoder[i] = count
        d_decoder[count] = i
        count += 1

print(d_encoder.values())
print(d_encoder.keys())

print(d_decoder.values())
print(d_decoder.keys())

# Print decoded labels
print(d_decoder.values())
print(d_decoder.keys())

# Function to encode labels
def encode_label(label):
    return d_encoder[label]
# Apply label encoding to the dataset
dataset['1'] = dataset['1'].apply(encode_label)

dataset['1'].head(2)

dataset['1'].unique()

# Tokenize the text data using Keras Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(dataset['0'])
sequences = tokenizer.texts_to_sequences(dataset['0'])
max_len = max([len(x) for x in sequences])

# Pad sequences to ensure uniform length
X_cnn = pad_sequences(sequences, maxlen=max_len)
# One-hot encode the labels
y_cnn = to_categorical(LabelEncoder().fit_transform(dataset['1']))

#train_texts, test_texts, train_labels, test_labels = train_test_split(dataset['0'].values, dataset['1'].values, test_size=0.2, random_state=42)

"""# Splitting the dataset"""

# Split the data into training and testing sets
X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y_cnn, test_size=0.2, random_state=42)

"""# Building the CNN model"""

# Building CNN-B 1D Model
model_cnn = Sequential()
model_cnn.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=500, input_length=max_len))
model_cnn.add(Conv1D(500, 5, activation='relu'))
model_cnn.add(GlobalMaxPooling1D())
model_cnn.add(Dense(50, activation='softmax'))

# Compile the CNN model with appropriate loss function, optimizer, and evaluation metric
model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Display the CNN model summary
print("CNN Model Summary:")
model_cnn.summary()

y_cnn.shape

"""# Training the model"""

# Training CNN Model
history_cnn = model_cnn.fit(X_train_cnn, y_train_cnn, epochs=15, batch_size=64, validation_data=(X_test_cnn, y_test_cnn))

"""# Evaluating the model"""

# Make predictions on the test data using the trained model
y_pred = model_cnn.predict(np.array(X_test_cnn))

# Convert predicted probabilities to binary predictions
y_pred=pd.DataFrame(y_pred)
y_pred=y_pred.eq(y_pred.where(y_pred != 0).max(1), axis=0).astype(int)
y_pred=y_pred.iloc[:,:].values

# Convert true labels to binary format
y_test=pd.DataFrame(y_test_cnn)
y_test=y_test.eq(y_test.where(y_test != 0).max(1), axis=0).astype(int)
y_test=y_test.iloc[:,:].values

# Initialize lists to store true and predicted labels

result=[]
for i in range(0,len(y_test)):
  for j in range(0,len(y_test[0])):
    if(y_test[i][j]==1):
      result.append(j)

predicted=[]
for i in range(0,len(y_pred)):
  for j in range(0,len(y_pred[0])):
    if(y_pred[i][j]==1):
      predicted.append(j)

print(result)
print(predicted)

# Compute and print accuracy, confusion matrix, and classification report

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(result,predicted)
print('Confusion Matrix :')
print(cm)
print('Accuracy Score :',accuracy_score(result, predicted))
print('Report : ')
print(classification_report(result, predicted))

"""# Visualization"""

# Plot loss curve
plt.plot(history_cnn.history["loss"], label="Loss")
plt.plot(history_cnn.history["val_loss"], label="Val_Loss")

plt.xlabel("Epochs")
plt.ylabel("Loss")

plt.title("Loss Vs Epochs -- > CNN Model")

plt.legend()
plt.grid()

# Plot accuracy curve
plt.plot(history_cnn.history["accuracy"], label="accuracy")
plt.plot(history_cnn.history["val_accuracy"], label="val_accuracy")

plt.xlabel("Epochs")
plt.ylabel("Accuracy")

plt.title("Accuracy Vs Epochs -- > CNN Model")

plt.legend()
plt.grid()