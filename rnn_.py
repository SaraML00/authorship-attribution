# -*- coding: utf-8 -*-
"""RNN .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WrKH4Y0JSE3WUyXrJNLI8Pra8n5rx4Zy
"""

# !pip install datasets transformers evaluate sentencepiece accelerate
!git clone https://github.com/chanchalIITP/AuthorIdentification.git

"""# Import necessary libraries and modules




"""

import numpy as np
import pandas as pd
import nltk, os
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer,PorterStemmer
from nltk.corpus import stopwords
from nltk import pos_tag
import re, string
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D, SimpleRNN
from keras.utils import to_categorical
from sklearn.metrics import accuracy_score, classification_report
from nltk import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC

"""Loading Data"""

# Load the dataset from GitHub
files_path = r'/content/AuthorIdentification/Dataset/Dataset_with_varying_number_of_tweets/1000_tweets_per_user.csv'
# Read the dataset from a CSV file
dataset = pd.read_csv(files_path)

dataset.head()

dataset.shape

dataset.iloc[0][0]

nltk.download('stopwords')
nltk.download('wordnet')

"""# Preprocesing"""

nltk.download('stopwords')
nltk.download('wordnet')
#Preprocesses a sentence by converting it to lowercase.
def preprocess(sentence):
    sentence=str(sentence)
    sentence = sentence.lower()
    return sentence

# Apply preprocessing to the text data
dataset.iloc[:, 0] = dataset.iloc[:,0].map(lambda s:preprocess(s))

dataset.iloc[:,0][0]

len(dataset['1'].unique()), dataset['1'].unique()

# Encode the labels (classes) in the dataset using integer encoding
classes = dataset['1'].unique()

d_encoder = {}
d_decoder = {}

count = 0
for i in classes:
    if i not in d_encoder.keys():
        d_encoder[i] = count
        d_decoder[count] = i
        count += 1



print(d_encoder.values())
print(d_encoder.keys())

print(d_decoder.values())
print(d_decoder.keys())

dataset['1'].head(2)

# Encode labels function
"""
    Encodes a label using a dictionary.

    Args:
    label (str): The label to be encoded.

    Returns:
    int: The encoded label.
    """
def encode_label(label):
    return d_encoder[label]

# Apply label encoding to the dataset

dataset['1'] = dataset['1'].apply(encode_label)

dataset['1'].head(2)

dataset['1'].unique()

#train_texts, test_texts, train_labels, test_labels = train_test_split(dataset['0'].values, dataset['1'].values, test_size=0.2, random_state=42)

#train_labels[:5]

"""# Embeding For RNN"""

# Tokenize the text data and pad sequences

tokenizer = Tokenizer()
tokenizer.fit_on_texts(dataset['0'])
sequences = tokenizer.texts_to_sequences(dataset['0'])
max_len = max([len(x) for x in sequences])

X_rnn = pad_sequences(sequences, maxlen=max_len)
y_rnn = to_categorical(LabelEncoder().fit_transform(dataset['1']))

# Split the dataset into training and testing sets

X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X_rnn, y_rnn, test_size=0.2, random_state=42)

y_rnn.shape

"""## RNN - Model"""

hidden_units = 300
dropout_rate = 0.5
learning_rate = 0.01
batch_size = 64
epochs = 15

from tensorflow.keras.layers import Bidirectional, SimpleRNN

# Define hyperparameters
hidden_units = 300
#dropout_rate = 0.2

# Build the RNN model
model_rnn = Sequential()
model_rnn.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=300))
model_rnn.add(Bidirectional(SimpleRNN(units=hidden_units, dropout=dropout_rate, return_sequences=True)))
model_rnn.add(Bidirectional(SimpleRNN(units=hidden_units, dropout=dropout_rate)))
model_rnn.add(Dense(units=50, activation='softmax'))

# Compile the model
model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Display the summary of the RNN model architecture
model_rnn.summary()

X_train_rnn, X_val_rnn, y_train_rnn, y_val_rnn = train_test_split(X_train_rnn, y_train_rnn, test_size=0.2, random_state=42)

# Training the RNN model on the training data
history_rnn = model_rnn.fit(X_train_rnn, y_train_rnn, epochs=20, batch_size=64,  validation_data=(X_test_rnn, y_test_rnn))

# Predict the classes for the test data
y_pred = model_rnn.predict(np.array(X_test_rnn))

y_pred=pd.DataFrame(y_pred)
y_pred=y_pred.eq(y_pred.where(y_pred != 0).max(1), axis=0).astype(int)
y_pred=y_pred.iloc[:,:].values

y_test=pd.DataFrame(y_test_rnn)
y_test=y_test.eq(y_test.where(y_test != 0).max(1), axis=0).astype(int)
y_test=y_test.iloc[:,:].values

# Convert the predicted and true labels back to their original format
result=[]
for i in range(0,len(y_test)):
  for j in range(0,len(y_test[0])):
    if(y_test[i][j]==1):
      result.append(j)

predicted=[]
for i in range(0,len(y_pred)):
  for j in range(0,len(y_pred[0])):
    if(y_pred[i][j]==1):
      predicted.append(j)

print(result)
print(predicted)

# Calculate accuracy, confusion matrix, and classification report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

accuracy = accuracy_score(result, predicted)
cm = confusion_matrix(result, predicted)
report = classification_report(result, predicted)

# Print the evaluation results
print('Confusion Matrix :')
print(cm)
print('Accuracy Score :',accuracy)
print('Classification_report')
print(report)

# Write the results to a file
with open('results.txt', 'w') as file:
    file.write('Confusion Matrix:\n')
    file.write(str(cm) + '\n\n')
    file.write('Accuracy Score: ' + str(accuracy) + '\n\n')
    file.write('Classification Report:\n')
    file.write(report)

plt.plot(history_rnn.history["accuracy"], label="accuracy")
plt.plot(history_rnn.history["val_accuracy"], label="val_accuracy")

plt.xlabel("Epochs")
plt.ylabel("Accuracy")

plt.title("Accuracy Vs Epochs -- > RNN Model")

plt.legend()
plt.grid()

plt.plot(history_rnn.history["loss"], label="Loss")
plt.plot(history_rnn.history["val_loss"], label="Val_Loss")

plt.xlabel("Epochs")
plt.ylabel("Loss")

plt.title("Loss Vs Epochs -- > RNN Model")

plt.legend()
plt.grid()